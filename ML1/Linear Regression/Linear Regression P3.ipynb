{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is OLS?\n",
    "\n",
    "What is objective function and why cost function is selected as objective function for Linear regression?\n",
    "\n",
    "How to find $Y_{pred}$ ?\n",
    "\n",
    "\n",
    "Answer : [Link](http://faculty.cas.usf.edu/mbrannick/regression/regbas.html)\n",
    "\n",
    "**RSS - Residual Sum of Squares**\n",
    "\n",
    "In the previous example of marketing spend (in lakhs) and sales amount (in crores), let’s assume you get the same data in different units — marketing spend (in lakhs) and sales amount (in dollars). Do you think there will be any change in the value of RSS due to change in units in this case (as compared to the value calculated in the Excel demonstration)?\n",
    "\n",
    "\n",
    "Yes, value of RSS would change because units are changing.\n",
    "Feedback :\n",
    "The RSS for any regression line is given by this expression: $ \\sum_(y_{i}-y_{ipred})^2$.\n",
    "\n",
    "RSS is the sum of the squared difference between the actual and the predicted values, and its value will change if the units change since it has units of $y^2$. For example, $(140 rupees - 70 rupees)^2 = 4900 $, whereas $(2 USD - 1 USD)^2 = 1$. So value of RSS is different in both the cases because of different units.\n",
    "\n",
    "\n",
    ">Relative vs absolute?\n",
    " * RSS- Absolute\n",
    " * TSS-Relative\n",
    " \n",
    "RSS and RSE is an absolute quantity and it depends on scale of y.\n",
    " \n",
    " \n",
    "We can build a model wthout independent variable which will be very basic model and any model which will build using independent variable should be better than this model.\n",
    "\n",
    "$ \\frac{TSS}{RSS}$ will help you to decide how good a model is\n",
    "\n",
    "$$TSS = \\sum_{i=1}^n(Y_{i}-\\bar{Y})^2$$\n",
    "\n",
    "\n",
    "$$ R^2 = 1- \\frac{RSS}{TSS}$$\n",
    "\n",
    "### $R^2 = 0.60 $ means we are able to explain 60% of the variance that is their in the data \n",
    "higher the value of $R^2$ better the model will be.\n",
    "\n",
    "Apart from **R²**, there is one more quantity named RSE (Residual Square Error) which is linked to RSS. Let’s see what that is.\n",
    "\n",
    "$$ RSE = \\sqrt{\\frac{RSS}{df}}$$\n",
    "\n",
    "df = n-2 where n = number of data points\n",
    "\n",
    "## Summary\n",
    "\n",
    "The equation of the best fit regression line Y = β₀ + β₁X can be found by minimising the **cost function (RSS in this case, using the Ordinary Least Squares method)** which is done using the following two methods:\n",
    "Differentiation\n",
    "Gradient descent method\n",
    "The strength of a linear regression model is mainly explained by R²,  where R² = 1 - (RSS / TSS)\n",
    "\n",
    "RSS: Residual Sum of Squares\n",
    "TSS: Total Sum of Squares\n",
    "\n",
    "\n",
    "---\n",
    "Hence, every time you perform a linear regression, you need to test whether the fitted line is a significant one or not or to simply put it, you need to test whether $\\beta_{1}$ is significant or not. And in comes the idea of Hypothesis Testing on $\\beta_{1}$\n",
    "\n",
    "You start by saying that $\\beta_{1}$ is not significant, i.e. there is no relationship between X and y.\n",
    "\n",
    "So in order to perform the hypothesis test, we first propose the null hypothesis that $\\beta_{1}$ is 0. And the alternative hypothesis thus becomes \n",
    "$\\beta_{1}$ is not zero.\n",
    "\n",
    "Null Hypothesis($H_{0}$):  $\\beta_{1}= 0 $\n",
    "\n",
    "Alternate Hypothesis($H_{A}$) : $\\beta_{1} \\neq 0 $\n",
    "\n",
    "\n",
    "\n",
    "Let's first discuss the implications of this hypothesis test. If you fail to reject the null hypothesis that would mean that $\\beta_{1}$ is zero which would simply mean that $\\beta_{1}$ is insignificant and of no use in the model. Similarly, if you reject the null hypothesis, it would mean that $\\beta_{1}$ is not zero and the line fitted is a significant one.\n",
    "\n",
    "Now, how do you perform the hypothesis test? Recall from your hypothesis testing module that you first used to compute the t-score (which is very similar to the Z-score) which is given by \n",
    "where $$ \\frac{X-\\mu}{{\\frac{s}{\\sqrt{n}}}}$$\n",
    "μ is the population mean and s is the sample standard deviation which when divided by √n\n",
    "is also known as standard error.\n",
    "\n",
    "Using this, the t-score for $\\hat{\\beta}$ comes out to be (since the null hypothesis is that $\\beta_{1}$ is equal to zero):\n",
    "\n",
    "$$ \\frac{\\hat{\\beta_{1}}-0}{SE(\\hat{\\beta_{1}})}$$\n",
    "\n",
    "\n",
    "Now, in order to perform the hypothesis test, you need to derive the p-value for the given beta. If you're hazy on what p-value is and how it is calculated, it is recommended that you revisit the segment on p-value. Please note that the formula of $ SE(\\hat{\\beta_{1}})$ provided in the t-score above is out of scope of this course.\n",
    "\n",
    "\n",
    "Let's do a quick recap of how do you calculate p-value anyway:\n",
    "\n",
    "Calculate the value of t-score for the mean point (in this case, zero, according to the Null hypothesis that we have stated) on the distribution\n",
    "Calculate the p-value from the cumulative probability for the given t-score using the t-table\n",
    "Make the decision on the basis of the p-value with respect to the given value of $\\beta$ (significance level)\n",
    "Now, if the p-value turns out to be less than 0.05, you can reject the null hypothesis and state that  $\\beta$ is indeed significant.\n",
    "\n",
    "#### Why p-value threshold for variable significance or insignificance is considered as 0.05?\n",
    "\n",
    "-------\n",
    "> The t-statistic along with the t-distribution table is used to determine the p-value of the coefficient.\n",
    "-------\n",
    "\n",
    "\n",
    "T-score\n",
    "Suppose that for a linear model, you got $\\beta_{1}$ as 0.5. Also, the standard error of  $\\beta_{1}$ was found out to be 0.02. What will be the value of t-score for $\\beta_{1}$ ?\n",
    "\n",
    "**Solution**\n",
    "\n",
    "Recall that the t-score for $\\beta_{1}$ is given as \n",
    "$$ \\frac{\\hat{\\beta_{1}}}{SE(\\hat{\\beta_{1}})}$$\n",
    "\n",
    "Hence, you have:\n",
    "\n",
    "t-score = $\\frac{0.5}{0.02} = 25$\n",
    "\n",
    "Significance of Beta\n",
    "From the t-score you got in the previous question, what can you say about the significance of $\\beta_{1}$?\n",
    "\n",
    "Recall that a t-distribution is very similar to a normal distribution. And a value as big as 25 means a practically zero p-value which in turn means that the variable is significant. You can have a look at the t-table [here](http://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf) anyway.\n",
    "\n",
    "\n",
    "#### Why does the test statistic for $\\beta_{1}$ follow a t-distribution instead of a normal distribution?\n",
    "\n",
    "[Link](https://www.youtube.com/watch?v=78YNvrsRzVw&t=269s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Linear Model\n",
    "\n",
    "### Summary Statistics\n",
    "------\n",
    "**F-statistic**\n",
    "\n",
    "You were introduced to a new term named F-statistic and Prob(F-statistic). Now, recall that in the last segment, you did a hypothesis test for beta to determine whether or not the coefficient \n",
    "$\\beta_{1}$ outputted by the model was significant or not. Now, F-statistic is similar in the sense that now instead of testing the significance of each of the betas, it tells you whether the overall model fit is significant or not. This parameter is examined because many a time it happens that even though all of your betas are significant, but your overall model fit might happen just by chance.\n",
    "\n",
    " \n",
    "The heuristic is similar to what you learnt in the normal p-value calculation as well. If the 'Prob (F-statistic)' is less than 0.05, you can conclude that the overall model fit is significant. If it is greater than 0.05, you might need to review your model as the fit might be by chance, i.e. the line may have just luckily fit the data. In the image above, you can see that the p-value of the F-statistic is 1.52e-52  which is practically a zero value. This means that the model for which this was calculated is definitely significant since it is less than 0.05.\n",
    "\n",
    " \n",
    "\n",
    "This will be more appreciable when you study multiple linear regression since there you have a lot of betas for the different predictor variables and thus there it is very helpful in determining if all the predictor variables together as a whole are significant or not or simply put, it tells you whether the model fit as a whole is significant or not.\n",
    "\n",
    "-----\n",
    "\n",
    "----\n",
    "**R-squared**\n",
    "\n",
    "Like you studied earlier as well, R-squared value tells you exactly how much variance in the data has been explained by the model. In our case, the R-squared is about 0.816 which means that the model is able to explain 81.6% of the variance which is pretty good.\n",
    "\n",
    "---\n",
    "\n",
    "**Coefficients and p-values:**\n",
    "\n",
    "The p-values of the coefficients (in this case just one coefficient for TV) tell you whether the coefficient is significant or not. In this case, the coefficient of TV came out to be 0.0545 with a standard error of about 0.002. Thus, you got a t-value of 24.722 which lead to a practically zero p-value. Hence, you can say that your coefficient is indeed significant. \n",
    "\n",
    "---\n",
    "\n",
    "Apart from this, the summary statistics outputs a few more metrics which are not of any use as of now. But you'll learn about some more of them in multiple linear regression.\n",
    "\n",
    "**Summary Statistics**\n",
    "\n",
    "Suppose you built a linear regression model in which the target variable is 'Scaled Pressure' which is being predicted with the help of the feature variable 'Frequency', and you got the following summary statistics of the model that you built.\n",
    "\n",
    "[Question pic](https://images.upgrad.com/8d7a7cfa-1f73-42c7-ae39-5365ad6630b0-summary.PNG)\n",
    "\n",
    "![Question pic](https://images.upgrad.com/8d7a7cfa-1f73-42c7-ae39-5365ad6630b0-summary.PNG)\n",
    "\n",
    "**Answer**\n",
    "\n",
    "The overall model fit is significant.\n",
    "If you look at the summary statistics, you can see that the F-statistic has a value of 270.2 which is a very high value and this, the Prob(F-statistic) is 5.93e-56 (as shown in the table) which is a practically zero value. Hence, the value of less than 0.05 which means that the overall model fit is significant.\n",
    "\n",
    "\n",
    "**Summary Statistics**\n",
    "\n",
    "\n",
    "Let's take a look at the summary statistics you saw in the last question again.\n",
    "\n",
    "What can you say about the significance of the coefficient the variable 'Frequency'?\n",
    "\n",
    "[link](https://images.upgrad.com/c34c4247-ba7e-4b12-afa9-04ca7e2dc8b5-summary.PNG)\n",
    "\n",
    "![link](https://images.upgrad.com/c34c4247-ba7e-4b12-afa9-04ca7e2dc8b5-summary.PNG)\n",
    "\n",
    "**Answer**\n",
    "\n",
    "Correct! If you look at the table, you can see that the p-value for the coefficient of the variable 'Frequency' is 0 which is a low value and hence, the coefficient is significant.\n",
    "\n",
    "[Link](https://images.upgrad.com/62179bac-52de-4c9a-897d-1bd6343c7f92-summary%20edited.png)\n",
    "\n",
    "![Link](https://images.upgrad.com/62179bac-52de-4c9a-897d-1bd6343c7f92-summary%20edited.png)\n",
    "\n",
    "\n",
    "**Summary Statistics**\n",
    "\n",
    "Finally, looking at the following summary statistics, what can you say about the extent of fit, i.e. the variance explanatory power of the model?\n",
    "\n",
    "[Link](https://images.upgrad.com/52d434b5-24f7-4391-9e91-5fde14d76ab6-summary.PNG)\n",
    "\n",
    "![Link](https://images.upgrad.com/52d434b5-24f7-4391-9e91-5fde14d76ab6-summary.PNG)\n",
    "\n",
    "**Answer**\n",
    "\n",
    "\n",
    "The R-squared value is low and hence, the model doesn't explain much of the variance.\n",
    "\n",
    "Correct! Look at the summary statistics closely. The value of R-squared is 0.153. Recall that R-squared varies from 0 to 1 wherein a value of 0 implies that none of the variance in the data is explained and a value of 1 implies that all of the variance in the data is explained. Can you answer the question now? Hence, a value of 0.153 is a low value of R-squared which in turn implies that the model doesn't explain much variance present in the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Residual Analysis and Predictions\n",
    "\n",
    "Residual Analysis and Predictions\n",
    "Now that you have built the linear model, you can move ahead with the next steps. Now, building the model on the train set has two parts - fitting a line and validating the assumptions of regression.\n",
    "\n",
    "\n",
    "Recall that one of the assumptions that you studied was that the error terms should be normally distributed with mean equal to 0. So once you have built the model, you'd need to verify if your model is not violating this assumption. And doing this is fairly simple - you just plot a histogram of the error terms to check whether they are normally distributed. And another assumption was that the error terms should be independent of each other. Again for this, you need to plot the error terms, this time with either of X or y to check for any patterns. Let's see all of this in action in the following video.\n",
    "\n",
    "\n",
    "### Assumptions of Simple Linear Regression\n",
    "\n",
    "While building a linear model, you assume that the target variable and the input variables are linearly dependent. But do you need any assumptions other than this?\n",
    "\n",
    "You are making inferences on the 'population' using a 'sample'. The assumption that variables are linearly dependent is not enough to generalise the results you obtain on a sample to the population, which is much larger in size than the sample. Thus, you need to have certain assumptions in place in order to make inferences.\n",
    "\n",
    "#### There is a linear relationship between X and Y\n",
    "\n",
    "> X and Y should display some sort of a linear relationship, otherwise, there is no use of fitting a linear model between them.\n",
    "\n",
    "[Link](https://images.upgrad.com/ae70c1b1-d094-4bbe-8adb-101f07f3b944-Linearity%20Assumption.jpg)\n",
    "\n",
    "![Link](https://images.upgrad.com/ae70c1b1-d094-4bbe-8adb-101f07f3b944-Linearity%20Assumption.jpg)\n",
    "\n",
    "\n",
    "> Error terms are normally distributed with mean zero(not X, Y)\n",
    " \n",
    " * There is no problem if the error terms are not normally distributed if you just wish to fit a line and not    make any further interpretations.\n",
    " * But if you're willing to make some inferences on the model you've built (you'll see this in further       \n",
    "   segments), you need to have a notion of distribution of the error terms. One particular repercussion of the    error terms not being normally distributed is that the p-values obtained during the hypothesis test to    \n",
    "   determine the significance of the coefficients become unreliable (you'll see this in a later segment.).\n",
    " * The assumption of normality is taken since it has been observed that the error terms generally follow a    \n",
    "   normal distribution with mean equal to zero in most of the cases.\n",
    "   [Link](https://images.upgrad.com/3bf3c52f-9b79-43a0-af1a-c9bd9287f33f-Normality%20Assumption.jpg)\n",
    "   ![Link](https://images.upgrad.com/3bf3c52f-9b79-43a0-af1a-c9bd9287f33f-Normality%20Assumption.jpg)\n",
    "   \n",
    ">  Error terms are independent of each other\n",
    "\n",
    "  * The error terms should not be dependent on one another (like in a time-series data wherein the next value     is dependent on the previous one).\n",
    "  \n",
    "[Link](https://images.upgrad.com/7837bb90-2fb3-4d23-89d4-c410ab57f669-Error%20Terms%20Independent%20Assumption.jpg)\n",
    "![Link](https://images.upgrad.com/7837bb90-2fb3-4d23-89d4-c410ab57f669-Error%20Terms%20Independent%20Assumption.jpg)\n",
    "\n",
    "> Error terms have constant variance (homoscedasticity)\n",
    "\n",
    " * The variance shouldn't increase (or decrease) as the error values change.\n",
    " * Also, the variance shouldn't follow any pattern as the error terms change.\n",
    " \n",
    " [Link](https://images.upgrad.com/31f8d6b9-1dbf-4742-848d-359c234b0c68-Constant%20Variance%20Assumption.jpg)\n",
    "  \n",
    " ![Link](https://images.upgrad.com/31f8d6b9-1dbf-4742-848d-359c234b0c68-Constant%20Variance%20Assumption.jpg)\n",
    " \n",
    " \n",
    "## [Assumptions violation outcome](http://people.duke.edu/~rnau/testing.htm)\n",
    " \n",
    "## Summary\n",
    "\n",
    "### Assumptions of simple linear regression\n",
    "\n",
    "* Linear relationship between X and y.\n",
    "* Normal distribution of error terms.\n",
    "* Independence of error terms.\n",
    "* Constant variance of error terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
